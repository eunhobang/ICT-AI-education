{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3_Softmax_Regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eunhobang/ICT-AI-education/blob/main/3_Softmax_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Softmax Classification"
      ],
      "metadata": {
        "id": "208Qkgubd6WA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 전체 학습코드"
      ],
      "metadata": {
        "id": "4E5yEigvKDSf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tB4W6HmPdvR9",
        "outputId": "cda03389-0301-47f2-ba22-402e5a2458a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "*****************running****************\n",
            "\n",
            "Run_count :[250] cost : [1.0696] \n",
            "pred : [[6.0205126e-01 1.5318877e-01 2.4475993e-01]\n",
            " [6.7362988e-01 2.1651680e-02 3.0471841e-01]\n",
            " [3.2034808e-07 9.9306113e-01 6.9384943e-03]\n",
            " [2.2542648e-01 5.4121411e-01 2.3335941e-01]\n",
            " [9.8537552e-01 1.8177211e-04 1.4442812e-02]\n",
            " [4.1336074e-02 2.6909071e-01 6.8957323e-01]] \n",
            "pred_Y : [0 0 1 1 0 2] \n",
            "true_Y : [2 2 1 1 0 0] \n",
            "accuracy : 50.00%%               \n",
            "\n",
            "*****************running****************\n",
            "\n",
            "Run_count :[500] cost : [0.6926] \n",
            "pred : [[4.3839169e-01 1.7315650e-01 3.8845184e-01]\n",
            " [7.1324700e-01 8.8020759e-03 2.7795085e-01]\n",
            " [2.6545031e-05 9.9421728e-01 5.7561132e-03]\n",
            " [9.6446000e-02 6.6956991e-01 2.3398407e-01]\n",
            " [9.7405821e-01 4.6694742e-05 2.5895137e-02]\n",
            " [2.2393814e-01 1.5998185e-01 6.1607999e-01]] \n",
            "pred_Y : [0 0 1 1 0 2] \n",
            "true_Y : [2 2 1 1 0 0] \n",
            "accuracy : 50.00%%               \n",
            "\n",
            "*****************running****************\n",
            "\n",
            "Run_count :[750] cost : [0.4949] \n",
            "pred : [[2.8983000e-01 1.8137972e-01 5.2879030e-01]\n",
            " [6.5614849e-01 5.7931468e-03 3.3805838e-01]\n",
            " [4.0970440e-04 9.9547261e-01 4.1177422e-03]\n",
            " [4.2921647e-02 7.4170995e-01 2.1536843e-01]\n",
            " [9.3173742e-01 2.6952217e-05 6.8235576e-02]\n",
            " [4.1733029e-01 9.9142462e-02 4.8352724e-01]] \n",
            "pred_Y : [2 0 1 1 0 2] \n",
            "true_Y : [2 2 1 1 0 0] \n",
            "accuracy : 66.67%%               \n",
            "\n",
            "*****************running****************\n",
            "\n",
            "Run_count :[1000] cost : [0.3959] \n",
            "pred : [[2.01899722e-01 1.79468408e-01 6.18631899e-01]\n",
            " [6.03056371e-01 4.42311540e-03 3.92520577e-01]\n",
            " [2.67662597e-03 9.94425535e-01 2.89777643e-03]\n",
            " [2.18419712e-02 7.85164297e-01 1.92993790e-01]\n",
            " [8.77222240e-01 1.70402091e-05 1.22760646e-01]\n",
            " [5.58879972e-01 6.80281669e-02 3.73091906e-01]] \n",
            "pred_Y : [2 0 1 1 0 0] \n",
            "true_Y : [2 2 1 1 0 0] \n",
            "accuracy : 83.33%%               \n",
            "Optimization Finished!\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "learning_rate = 0.01\n",
        "training_cnt = 1000\n",
        "display_step = 250\n",
        "\n",
        "X = np.array([[2, 3, 1, 1], [4, 3, 2, 1], [4, 2, 5, 5], [2, 5, 2, 1], [6, 3, 1, 2], [5, 4, 4, 2] ], dtype=np.float32)\n",
        "Y = np.array([[0, 0, 1], [0, 0 ,1], [0, 1, 0], [0, 1, 0], [1, 0, 0 ], [1, 0 ,0]])\n",
        "\n",
        "W = tf.Variable(tf.random.normal([4, 3]), name='weight') \n",
        "b = tf.Variable(tf.random.normal([3]), name='bias')  \n",
        "\n",
        "for epoch in range(training_cnt):\n",
        "  with tf.GradientTape() as tape:\n",
        "    pred = tf.nn.softmax(tf.matmul(X ,W) + b)\n",
        "    cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.math.log(pred), axis=1))\n",
        "  W_grad, b_grad = tape.gradient(cost, [W, b])\n",
        "  W.assign_sub(learning_rate * W_grad)\n",
        "  b.assign_sub(learning_rate * b_grad)\n",
        "  prediction = tf.argmax(pred, 1)\n",
        "  true_Y = tf.argmax(Y, 1)\n",
        "  accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, true_Y), dtype=tf.float32))\n",
        "  if (epoch+1) % display_step == 0:\n",
        "    print('\\n*****************running****************\\n')\n",
        "    print('Run_count :[{}] cost : [{:0.4f}] \\npred : {} \\npred_Y : {} \\ntrue_Y : {} \\naccuracy : {:.2%}% \\\n",
        "              ' .format( (epoch+1), cost, pred, prediction, true_Y, accuracy)\n",
        "             )\n",
        "print(\"Optimization Finished!\") "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) 파라메터 값 설정\n",
        "- 머신러닝을 위한 기초 파라메터\n",
        "- learning_rate : 값이 너무 적으면 Train 되지 않을 수 있고, 값이 너무 크면 overshooting이 발생할 수 있다.\n",
        "- training_cnt : data set에 대한 training 반복 횟수\n"
      ],
      "metadata": {
        "id": "4n_LIn7sgAXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 파라메터값 설정\n",
        "learning_rate = 0.01\n",
        "training_cnt = 1000\n",
        "display_step = 250  # 원하는 출력 위치 조정"
      ],
      "metadata": {
        "id": "pf2taqNCcwJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2) 트레이닝 데이터 변수 선언\n",
        "- 입력으로 들어가는 x data(input 4개), y data(output 3개) 설정\n",
        "- 레이블 데이터를 one-hot encoding 형태로 구성"
      ],
      "metadata": {
        "id": "rWTWgaEZg1RS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([[2, 3, 1, 1], [4, 3, 2, 1], [4, 2, 5, 5], [2, 5, 2, 1], [6, 3, 1, 2], [5, 4, 4, 2] ], dtype=np.float32)\n",
        "Y = np.array([[0, 0, 1], [0, 0 ,1], [0, 1, 0], [0, 1, 0], [1, 0, 0 ], [1, 0 ,0]])"
      ],
      "metadata": {
        "id": "WPSKavZog0TF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) tf.random.normal\n",
        "- bias, weight 초기값을 난수로 생성"
      ],
      "metadata": {
        "id": "_gyQXl29g_I9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W = tf.Variable(tf.random.normal([4, 3]), name='weight')\n",
        "b = tf.Variable(tf.random.normal([3]), name='bias')"
      ],
      "metadata": {
        "id": "nZTT4bwQgdJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4) softmax 함수 사용\n",
        "- 기존 pred 계산에 softmax 함수를 적용하여 출력값의 합이 항상 1이 되게 한다."
      ],
      "metadata": {
        "id": "xttKfjo3ha_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred = tf.nn.softmax(tf.matmul(X ,W) + b)"
      ],
      "metadata": {
        "id": "Z6y1CoQ4hcjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5) cost function\n",
        "- 교차 엔트로피(cross-entropy) 사용\n",
        "- 예측값과 실제값 사이의 확률분포 차이 계산"
      ],
      "metadata": {
        "id": "x9vGezd5hfSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.math.log(pred), axis=1))"
      ],
      "metadata": {
        "id": "crWnJH5Qhejw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6) 학습\n",
        "- gradient decent 함수 사용(경사 하강법) \n",
        "-  파라미터 $W$, $b$ 에 대해 손실을 미분하는 과정으로, 파라미터를 증가시킬 때 손실이 얼마나 변화하는지를 알아본다. \n",
        "- with tf.GradientTape() as tape: 안에서 계산을 하면 tape에 계산 과정을 기록해두었다가 tape.gradient를 이용해서 미분을 자동으로 구할 수 있다"
      ],
      "metadata": {
        "id": "qdtBqqWjhv2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.GradientTape() as tape:\n",
        "  pred = tf.nn.softmax(tf.matmul(X ,W) + b)\n",
        "  cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.math.log(pred), axis=1))\n",
        "W_grad, b_grad = tape.gradient(cost, [W, b])"
      ],
      "metadata": {
        "id": "hr3To06WKg8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- $w←w−η∂w$ 의 식으로 파라미터를 수정\n",
        "- $η$ 는 학습률\n",
        "- 경사(미분)을 따라 손실을 줄여나가기 때문에 경사하강법이라고 부름\n",
        "- a.assign_sub(b)는 a = a - b 와 같다"
      ],
      "metadata": {
        "id": "pcb8bNEZQ43s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W.assign_sub(learning_rate * W_grad)\n",
        "b.assign_sub(learning_rate * b_grad)"
      ],
      "metadata": {
        "id": "1adf3vd9hwnd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bb4d93d-7626-4f7c-93a0-d599a9150396"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'UnreadVariable' shape=(3,) dtype=float32, numpy=array([-0.04315853,  0.4439231 ,  1.3830305 ], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7) 학습된 예측 값을 확인\n",
        "- 예측된 최대 값의 index 반환\n",
        "- One-hot encoding 한 Y값도 최대값 1이 있는 index 반환"
      ],
      "metadata": {
        "id": "HfkFWOXMiDMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = tf.argmax(pred, 1)\n",
        "true_Y = tf.argmax(Y, 1)"
      ],
      "metadata": {
        "id": "FlDq_m9Yh_WU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8) 정확도\n",
        "- accuracy를 계산하여 분류가 정확한지 확인\n",
        "- 예측값과 실제 데이터의 일치 여부 계산 \n",
        "- 아래 코드는 평균을 이용한 정확도 계산"
      ],
      "metadata": {
        "id": "6aUuhStAiNm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, true_Y), dtype=tf.float32))"
      ],
      "metadata": {
        "id": "yOQ4qPbIiLk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (2) 모델 실행(run/update)\n",
        "- pred는 softmax 함수를 통해 0~1사이의 값으로 나온다\n",
        "- pred_Y는 pred에서 나온 최대 예측값의 index 반환\n",
        "- accuracy는 예측한 Y값이 실제 Y값과 얼마나 일치하는가"
      ],
      "metadata": {
        "id": "fRmQg_XjiURK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(training_cnt):\n",
        "  with tf.GradientTape() as tape:\n",
        "    pred = tf.nn.softmax(tf.matmul(X ,W) + b)\n",
        "    cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.math.log(pred), axis=1))\n",
        "  W_grad, b_grad = tape.gradient(cost, [W, b])\n",
        "  W.assign_sub(learning_rate * W_grad)\n",
        "  b.assign_sub(learning_rate * b_grad)\n",
        "  prediction = tf.argmax(pred, 1)\n",
        "  true_Y = tf.argmax(Y, 1)\n",
        "  accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, true_Y), dtype=tf.float32))\n",
        "  if (epoch+1) % display_step == 0:\n",
        "    print('\\n*****************running****************\\n')\n",
        "    print('Run_count :[{}] cost : [{:0.4f}] \\npred : {} \\npred_Y : {} \\ntrue_Y : {} \\naccuracy : {:.2%}% \\\n",
        "              ' .format( (epoch+1), cost, pred, prediction, true_Y, accuracy)\n",
        "             )\n",
        "print(\"Optimization Finished!\") "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIDWPyZsiUsE",
        "outputId": "3845a022-d766-4b2a-8c64-0aa2065d2a26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "*****************running****************\n",
            "\n",
            "Run_count :[250] cost : [0.3673] \n",
            "pred : [[9.1995195e-02 3.7407082e-01 5.3393400e-01]\n",
            " [4.6268550e-01 1.6022481e-02 5.2129209e-01]\n",
            " [6.5296069e-02 9.3404078e-01 6.6322368e-04]\n",
            " [1.1801899e-02 7.2715425e-01 2.6104382e-01]\n",
            " [8.2805353e-01 1.1212676e-03 1.7082523e-01]\n",
            " [7.0495087e-01 5.6313571e-02 2.3873551e-01]] \n",
            "pred_Y : [2 2 1 1 0 0] \n",
            "true_Y : [2 2 1 1 0 0] \n",
            "accuracy : 100.00%%               \n",
            "\n",
            "*****************running****************\n",
            "\n",
            "Run_count :[500] cost : [0.3403] \n",
            "pred : [[7.9887114e-02 3.4866616e-01 5.7144678e-01]\n",
            " [4.4892907e-01 1.2625501e-02 5.3844547e-01]\n",
            " [6.0225118e-02 9.3942463e-01 3.5034199e-04]\n",
            " [8.2456162e-03 7.4479550e-01 2.4695884e-01]\n",
            " [8.2954794e-01 5.6853733e-04 1.6988346e-01]\n",
            " [7.2661328e-01 5.4354053e-02 2.1903269e-01]] \n",
            "pred_Y : [2 2 1 1 0 0] \n",
            "true_Y : [2 2 1 1 0 0] \n",
            "accuracy : 100.00%%               \n",
            "\n",
            "*****************running****************\n",
            "\n",
            "Run_count :[750] cost : [0.3173] \n",
            "pred : [[7.0041418e-02 3.2587922e-01 6.0407931e-01]\n",
            " [4.3518907e-01 1.0092147e-02 5.5471885e-01]\n",
            " [5.6197155e-02 9.4360524e-01 1.9768941e-04]\n",
            " [5.8867009e-03 7.6083809e-01 2.3327516e-01]\n",
            " [8.3345526e-01 3.0004344e-04 1.6624466e-01]\n",
            " [7.4325526e-01 5.2479688e-02 2.0426504e-01]] \n",
            "pred_Y : [2 2 1 1 0 0] \n",
            "true_Y : [2 2 1 1 0 0] \n",
            "accuracy : 100.00%%               \n",
            "\n",
            "*****************running****************\n",
            "\n",
            "Run_count :[1000] cost : [0.2972] \n",
            "pred : [[6.1928984e-02 3.0533546e-01 6.3273555e-01]\n",
            " [4.2163733e-01 8.1685875e-03 5.7019407e-01]\n",
            " [5.2887380e-02 9.4699579e-01 1.1689364e-04]\n",
            " [4.2920969e-03 7.7540535e-01 2.2030261e-01]\n",
            " [8.3848065e-01 1.6438821e-04 1.6135494e-01]\n",
            " [7.5673938e-01 5.0713282e-02 1.9254737e-01]] \n",
            "pred_Y : [2 2 1 1 0 0] \n",
            "true_Y : [2 2 1 1 0 0] \n",
            "accuracy : 100.00%%               \n",
            "Optimization Finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 기타 학습 방법\n",
        "- 출력 Y를 one_hot 함수를 이용하여 encoding 한 후 학습\n",
        "- one_hot으로 늘어난 차원을 reshape로 축소\n",
        "- softmax_cross_entropy_with_logits 함수 이용"
      ],
      "metadata": {
        "id": "yLm3s431ij_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "learning_rate = 0.01\n",
        "training_cnt = 1000\n",
        "display_step = 250\n",
        "\n",
        "X = np.array([[2, 3, 1, 1], [4, 3, 2, 1], [4, 2, 5, 5], [2, 5, 2, 1], [6, 3, 1, 2], [5, 4, 4, 2] ], dtype = np.float32)\n",
        "Y = np.array([[2], [2], [1], [1], [0], [0]])\n",
        "\n",
        "Y_1 = tf.one_hot(Y, 3)  \n",
        "Y_2 = tf.reshape(Y_1, [-1, 3]) \n",
        "\n",
        "W = tf.Variable(tf.random.normal([4, 3]), name='weight') \n",
        "b = tf.Variable(tf.random.normal([3]), name='bias')  \n",
        "\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  logits = tf.matmul(X ,W) + b\n",
        "  cost_i = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = Y_2)\n",
        "  cost = tf.reduce_mean(cost_i)\n",
        "\n",
        "W_grad, b_grad = tape.gradient(cost, [W, b])\n",
        "W.assign_sub(learning_rate * W_grad)\n",
        "b.assign_sub(learning_rate * b_grad)\n",
        "\n",
        "print('Y : {} \\none_hot_Y: {} \\none_hot_reshape_Y {} ' .format(Y, Y_1,Y_2))\n",
        "\n",
        "for epoch in range(training_cnt):\n",
        "    with tf.GradientTape() as tape:\n",
        "      logits = tf.matmul(X ,W) + b\n",
        "      cost_i = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = Y_2)\n",
        "      cost = tf.reduce_mean(cost_i)\n",
        "    W_grad, b_grad = tape.gradient(cost, [W, b])\n",
        "    W.assign_sub(learning_rate * W_grad)\n",
        "    b.assign_sub(learning_rate * b_grad)\n",
        "    pred = tf.nn.softmax(logits)\n",
        "    prediction = tf.argmax(pred, 1)\n",
        "    true_Y = tf.argmax(Y_2, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, true_Y), dtype=tf.float32))\n",
        "    \n",
        "    if (epoch+1) % display_step == 0:\n",
        "      print('\\n*****************running****************\\n')\n",
        "      print('Run_count :[{}] cost : [{:0.4f}] \\npred : {} \\npred_Y : {} \\ntrue_Y : {} \\naccuracy : {:.2%}% \\\n",
        "              ' .format( (epoch+1), cost, pred, prediction, true_Y, accuracy)\n",
        "             )\n",
        "      \n",
        "     \n",
        "\n",
        "print(\"Optimization Finished!\") "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yb3pQZcLikr0",
        "outputId": "f32297b7-ef73-46a9-894b-3991ed9e5764"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y : [[2]\n",
            " [2]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]] \n",
            "one_hot_Y: [[[0. 0. 1.]]\n",
            "\n",
            " [[0. 0. 1.]]\n",
            "\n",
            " [[0. 1. 0.]]\n",
            "\n",
            " [[0. 1. 0.]]\n",
            "\n",
            " [[1. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0.]]] \n",
            "one_hot_reshape_Y [[0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]] \n",
            "\n",
            "*****************running****************\n",
            "\n",
            "Run_count :[250] cost : [0.6630] \n",
            "pred : [[0.05822507 0.60025233 0.3415226 ]\n",
            " [0.5806115  0.1293246  0.2900639 ]\n",
            " [0.11804385 0.8010038  0.08095237]\n",
            " [0.00595409 0.52335405 0.4706919 ]\n",
            " [0.9819742  0.01070641 0.00731934]\n",
            " [0.45915365 0.13660136 0.40424496]] \n",
            "pred_Y : [1 0 1 1 0 0] \n",
            "true_Y : [2 2 1 1 0 0] \n",
            "accuracy : 66.67%%               \n",
            "\n",
            "*****************running****************\n",
            "\n",
            "Run_count :[500] cost : [0.5123] \n",
            "pred : [[0.05718353 0.5176505  0.42516595]\n",
            " [0.52299356 0.06895029 0.40805614]\n",
            " [0.10107205 0.875276   0.02365191]\n",
            " [0.00696773 0.595101   0.39793122]\n",
            " [0.96264184 0.00455655 0.03280165]\n",
            " [0.53161216 0.09973872 0.36864907]] \n",
            "pred_Y : [1 0 1 1 0 0] \n",
            "true_Y : [2 2 1 1 0 0] \n",
            "accuracy : 66.67%%               \n",
            "\n",
            "*****************running****************\n",
            "\n",
            "Run_count :[750] cost : [0.4298] \n",
            "pred : [[0.05584592 0.46017382 0.48398018]\n",
            " [0.48362476 0.04175854 0.47461662]\n",
            " [0.09086533 0.90001965 0.009115  ]\n",
            " [0.00747539 0.65893525 0.33358932]\n",
            " [0.91767484 0.0020749  0.08025022]\n",
            " [0.60693353 0.0791446  0.3139219 ]] \n",
            "pred_Y : [2 0 1 1 0 0] \n",
            "true_Y : [2 2 1 1 0 0] \n",
            "accuracy : 83.33%%               \n",
            "\n",
            "*****************running****************\n",
            "\n",
            "Run_count :[1000] cost : [0.3821] \n",
            "pred : [[0.05406795 0.41504863 0.5308835 ]\n",
            " [0.46493495 0.0278929  0.5071722 ]\n",
            " [0.07970894 0.9162809  0.00401012]\n",
            " [0.00718389 0.6996135  0.29320267]\n",
            " [0.87614506 0.00094503 0.12290993]\n",
            " [0.66777754 0.06675304 0.26546946]] \n",
            "pred_Y : [2 2 1 1 0 0] \n",
            "true_Y : [2 2 1 1 0 0] \n",
            "accuracy : 100.00%%               \n",
            "Optimization Finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) 정수로들어온 클래스 one_hot encoding 하기\n",
        "- 0,1,2,3,.... 의 정수 클래스를 one_hot 함수 사용하여 변경\n",
        "- one_hot을 하면 차원이 +1 이 된다.\n",
        "- reshape를 통해 차원을 다시 맞춰준다."
      ],
      "metadata": {
        "id": "jMfPnca2mv1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Y = np.array([[2], [2], [1], [1], [0], [0]], dtype=np.float32)\n",
        "\n",
        "Y_1 = tf.one_hot(Y, 3)  \n",
        "Y_2 = tf.reshape(Y_1, [-1, 3]) "
      ],
      "metadata": {
        "id": "LCBX7ZGHlTM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) cost 함수 사용\n",
        "- softmax_cross_entropy_with_logits_v2 함수 이용\n",
        "- softmax_cross_entropy_with_logits 함수의 업그레이드 버전\n",
        "- one_hot encoding한 라벨값 사용"
      ],
      "metadata": {
        "id": "QZBTJNOuoBnF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits = tf.matmul(X ,W) + b\n",
        "cost_i = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = Y_2)\n",
        "cost = tf.reduce_mean(cost_i)"
      ],
      "metadata": {
        "id": "SNu9h6Qmm3_l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}